{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0700100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "import lucid\n",
    "import lucid.nn as nn\n",
    "import lucid.nn.functional as F\n",
    "import lucid.optim as optim\n",
    "\n",
    "from lucid._tensor import Tensor\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = lucid.zeros(max_len, d_model)\n",
    "        position = lucid.arange(0, max_len, dtype=lucid.Float32).unsqueeze(axis=1)\n",
    "        div_term = lucid.exp(\n",
    "            lucid.arange(0, d_model, 2, dtype=lucid.Float32)\n",
    "            * (-lucid.log(1e4) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = lucid.sin(position * div_term)\n",
    "        pe[:, 1::2] = lucid.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(axis=0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        seq_len = x.shape[1]\n",
    "        x += self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        dim_feedforward: int = 2048,\n",
    "        num_heads: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "        pad_id: int = 0,\n",
    "        tie_weights: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=pad_id)\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(d_model, dropout, max_len=5000)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(d_model, tgt_vocab_size, bias=not tie_weights)\n",
    "        if tie_weights:\n",
    "            self.out.weight = self.tgt_embedding.weight\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mask_padding_mask(tokens: Tensor, pad_id: int) -> Tensor:\n",
    "        return tokens == pad_id\n",
    "\n",
    "    @staticmethod\n",
    "    def _mask_square_subseq_mask(sz: int) -> Tensor:\n",
    "        return lucid.triu(lucid.full((sz, sz), -lucid.inf), diagonal=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        tgt: Tensor,\n",
    "        tgt_mask: Tensor | None = None,\n",
    "        src_pad_mask: Tensor | None = None,\n",
    "        tgt_pad_mask: Tensor | None = None,\n",
    "    ) -> Tensor:\n",
    "        device = src.device\n",
    "\n",
    "        src_emb = self.src_embedding(src) * lucid.sqrt(self.d_model)\n",
    "        tgt_emb = self.tgt_embedding(tgt) * lucid.sqrt(self.d_model)\n",
    "\n",
    "        src_emb = self.positional_encoder(src_emb)\n",
    "        tgt_emb = self.positional_encoder(tgt_emb)\n",
    "\n",
    "        if tgt_mask is None:\n",
    "            T = tgt_emb.shape[1]\n",
    "            tgt_mask = self._mask_square_subseq_mask(T).to(device)\n",
    "            tgt_mask = tgt_mask.astype(bool)\n",
    "\n",
    "        if src_pad_mask is None:\n",
    "            src_pad_mask = self._mask_padding_mask(src, self.pad_id)\n",
    "        if tgt_pad_mask is None:\n",
    "            tgt_pad_mask = self._mask_padding_mask(tgt, self.pad_id)\n",
    "\n",
    "        x = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            mem_key_padding_mask=src_pad_mask,\n",
    "        )\n",
    "\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6382f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerScheduler(optim.lr_scheduler.LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: optim.Optimizer,\n",
    "        d_model: int,\n",
    "        warmup_steps: int = 4000,\n",
    "        last_epoch: int = -1,\n",
    "    ) -> None:\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def get_lr(self) -> list[float]:\n",
    "        step = max(1, self._step_count)\n",
    "        scale = self.d_model**0.5\n",
    "\n",
    "        arg1 = step**-0.5\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        lr = scale * min(arg1, arg2)\n",
    "        return [lr for _ in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"data/tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "max_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fab4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n",
    "START_ID = tokenizer.token_to_id(\"[START]\")\n",
    "END_ID = tokenizer.token_to_id(\"[END]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
